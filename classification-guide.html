<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classification Guide - My AI Learning Blog</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <nav class="back-nav">
        <button onclick="window.location.href='ai.html'" class="back-btn">‚Üê Back to AI Hub</button>
    </nav>
    
    <header>
        <h1>üéØ Classification Guide</h1>
        <p>Complete guide to classification algorithms with examples and implementations</p>
    </header>

    <main>
        <!-- Overview Block -->
        <div class="blog-post">
            <div class="topic-header">
                <div class="topic-icon">üéØ</div>
                <div>
                    <h3>What is Classification?</h3>
                </div>
            </div>
            <p>Classification is a supervised learning technique used to predict discrete categories or classes. Unlike regression which predicts continuous values, classification assigns input data to specific predefined categories.</p>
            
            <div class="visual-box">
                <strong>Key Goal:</strong> Learn decision boundaries to separate different classes in the feature space.
            </div>

            <h4>Types of Classification:</h4>
            <ul>
                <li><strong>Binary Classification:</strong> Two classes (spam/not spam, yes/no)</li>
                <li><strong>Multi-class Classification:</strong> Multiple classes (cat/dog/bird)</li>
                <li><strong>Multi-label Classification:</strong> Multiple labels per instance</li>
            </ul>

            <h4>Common Applications:</h4>
            <ul>
                <li>Email spam detection</li>
                <li>Image recognition</li>
                <li>Medical diagnosis</li>
                <li>Sentiment analysis</li>
                <li>Credit approval</li>
                <li>Fraud detection</li>
            </ul>
        </div>

        <!-- Classification Algorithms Tabs -->
        <div class="blog-post">
            <div class="topic-header">
                <div class="topic-icon">ü§ñ</div>
                <div>
                    <h3>Classification Algorithms</h3>
                </div>
            </div>
            
            <div class="algorithm-tabs">
                <button class="tab-btn active" onclick="showClassificationTab('logistic-regression')">üìà Logistic Regression</button>
                <button class="tab-btn" onclick="showClassificationTab('knn')">üë• K-Nearest Neighbors</button>
                <button class="tab-btn" onclick="showClassificationTab('svm')">üéØ Support Vector Machine</button>
                <button class="tab-btn" onclick="showClassificationTab('kernel-svm')">üîÑ Kernel SVM</button>
                <button class="tab-btn" onclick="showClassificationTab('naive-bayes')">üß† Naive Bayes</button>
                <button class="tab-btn" onclick="showClassificationTab('decision-tree')">üå≥ Decision Tree</button>
                <button class="tab-btn" onclick="showClassificationTab('random-forest')">üå≤ Random Forest</button>
            </div>

            <div class="tab-content">
                <!-- Logistic Regression Tab -->
                <div id="logistic-regression" class="tab-panel active">
                    <h4>üìà Logistic Regression</h4>
                    <p>Uses the logistic function to model the probability of binary or categorical outcomes. Despite its name, it's a classification algorithm.</p>

                    <h4>Formula:</h4>
                    <div class="visual-box">
                        p = 1 / (1 + e^(-z))
                        <br><br>
                        Where z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô
                        <br><br>
                        ‚Ä¢ p = Probability of positive class
                        ‚Ä¢ e = Euler's number (‚âà2.718)
                        ‚Ä¢ Œ≤ = Coefficients learned during training
                    </div>

                    <h4>Decision Boundary:</h4>
                    <div class="visual-box">
                        If p ‚â• 0.5 ‚Üí Class 1
                        If p < 0.5 ‚Üí Class 0
                        <br><br>
                        Linear decision boundary: Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ = 0
                    </div>

                    <div class="visual-box">
                        <h4>Example: Email Spam Detection</h4>
                        <p><strong>Features:</strong> Number of exclamation marks, word count</p>
                        <div class="visual-box">
z = -2.5 + 0.8√óexclamations + 0.001√óword_count

Email: 5 exclamations, 200 words
z = -2.5 + 0.8√ó5 + 0.001√ó200 = -2.5 + 4 + 0.2 = 1.7
p = 1/(1 + e^(-1.7)) = 1/(1 + 0.183) = 0.845

Result: 84.5% probability of spam ‚Üí Classified as SPAM
                        </div>
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Provides probability estimates<br>
                                ‚Ä¢ No assumptions about feature distributions<br>
                                ‚Ä¢ Less prone to overfitting<br>
                                ‚Ä¢ Fast training and prediction<br>
                                ‚Ä¢ Interpretable coefficients
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Assumes linear relationship<br>
                                ‚Ä¢ Sensitive to outliers<br>
                                ‚Ä¢ Requires large sample sizes<br>
                                ‚Ä¢ Can struggle with complex relationships<br>
                                ‚Ä¢ Feature scaling needed
                            </div>
                        </div>
                    </div>
                </div>

                <!-- K-Nearest Neighbors Tab -->
                <div id="knn" class="tab-panel">
                    <h4>üë• K-Nearest Neighbors (KNN)</h4>
                    <p>A lazy learning algorithm that classifies data points based on the majority class of their k nearest neighbors in the feature space.</p>

                    <h4>Algorithm:</h4>
                    <div class="visual-box">
                        1. Choose number of neighbors (k)
                        2. Calculate distance to all training points
                        3. Find k nearest neighbors
                        4. Take majority vote of their classes
                        5. Assign predicted class
                    </div>

                    <h4>Distance Metrics:</h4>
                    <div class="visual-box">
                        Euclidean: d = ‚àö[(x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤]
                        <br><br>
                        Manhattan: d = |x‚ÇÅ-x‚ÇÇ| + |y‚ÇÅ-y‚ÇÇ|
                        <br><br>
                        Minkowski: d = (Œ£|x·µ¢-y·µ¢|^p)^(1/p)
                    </div>

                    <div class="visual-box">
                        <h4>Example: Flower Classification</h4>
                        <p><strong>Features:</strong> Petal length, petal width</p>
                        <div class="visual-box">
New flower: petal_length=5.1, petal_width=2.3

Distances to training data:
Point 1 (Setosa): d = ‚àö[(5.1-4.2)¬≤ + (2.3-1.5)¬≤] = 1.34
Point 2 (Versicolor): d = ‚àö[(5.1-5.0)¬≤ + (2.3-2.2)¬≤] = 0.14
Point 3 (Virginica): d = ‚àö[(5.1-5.8)¬≤ + (2.3-2.7)¬≤] = 0.84
Point 4 (Versicolor): d = ‚àö[(5.1-4.9)¬≤ + (2.3-2.0)¬≤] = 0.36
Point 5 (Virginica): d = ‚àö[(5.1-6.1)¬≤ + (2.3-2.8)¬≤] = 1.12

For k=3, nearest neighbors: Versicolor, Virginica, Versicolor
Majority vote: Versicolor ‚Üí Prediction: Versicolor
                        </div>
                    </div>

                    <h4>Choosing K:</h4>
                    <div class="visual-box">
                        ‚Ä¢ Small k: More sensitive to noise, complex boundaries
                        ‚Ä¢ Large k: Smoother boundaries, may lose local patterns
                        ‚Ä¢ Odd k: Avoids ties in binary classification
                        ‚Ä¢ Rule of thumb: k = ‚àön (where n = training samples)
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Simple to understand and implement<br>
                                ‚Ä¢ No assumptions about data distribution<br>
                                ‚Ä¢ Works well with small datasets<br>
                                ‚Ä¢ Can handle multi-class naturally<br>
                                ‚Ä¢ Adapts to new data easily
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Computationally expensive for large datasets<br>
                                ‚Ä¢ Sensitive to irrelevant features<br>
                                ‚Ä¢ Requires feature scaling<br>
                                ‚Ä¢ Poor performance with high dimensions<br>
                                ‚Ä¢ Memory intensive
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Support Vector Machine Tab -->
                <div id="svm" class="tab-panel">
                    <h4>üéØ Support Vector Machine (SVM)</h4>
                    <p>Finds the optimal hyperplane that separates classes with maximum margin. The hyperplane is determined by support vectors.</p>

                    <h4>Key Concepts:</h4>
                    <div class="visual-box">
                        ‚Ä¢ Hyperplane: Decision boundary separating classes
                        ‚Ä¢ Support Vectors: Data points closest to hyperplane
                        ‚Ä¢ Margin: Distance between hyperplane and nearest points
                        ‚Ä¢ Goal: Maximize margin for better generalization
                    </div>

                    <h4>Mathematical Formulation:</h4>
                    <div class="visual-box">
                        Hyperplane: w¬∑x + b = 0
                        <br><br>
                        Optimization: Minimize ¬Ω||w||¬≤
                        Subject to: y·µ¢(w¬∑x·µ¢ + b) ‚â• 1 for all i
                        <br><br>
                        ‚Ä¢ w = Weight vector (normal to hyperplane)
                        ‚Ä¢ b = Bias term
                        ‚Ä¢ y·µ¢ = Class label (+1 or -1)
                    </div>

                    <div class="visual-box">
                        <h4>Example: Binary Classification</h4>
                        <div class="visual-box">
Dataset: Customer credit approval
Features: Income, Credit Score

Training data:
Customer A: Income=50k, Score=700, Label=Approved (+1)
Customer B: Income=30k, Score=600, Label=Rejected (-1)
Customer C: Income=70k, Score=750, Label=Approved (+1)
Customer D: Income=25k, Score=550, Label=Rejected (-1)

SVM finds hyperplane: 0.001√óIncome + 0.002√óScore - 1.8 = 0

New customer: Income=60k, Score=680
Decision = 0.001√ó60000 + 0.002√ó680 - 1.8 = 60 + 1.36 - 1.8 = 59.56 > 0
Result: Approved
                        </div>
                    </div>

                    <h4>Soft Margin SVM:</h4>
                    <div class="visual-box">
                        For non-linearly separable data:
                        Minimize: ¬Ω||w||¬≤ + C‚àëŒæ·µ¢
                        <br><br>
                        ‚Ä¢ C = Regularization parameter
                        ‚Ä¢ Œæ·µ¢ = Slack variables (allow misclassification)
                        ‚Ä¢ Higher C = Less tolerance for misclassification
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Effective in high dimensions<br>
                                ‚Ä¢ Memory efficient (uses support vectors)<br>
                                ‚Ä¢ Versatile (different kernels)<br>
                                ‚Ä¢ Works well with clear margin<br>
                                ‚Ä¢ Robust to overfitting
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ No probabilistic output<br>
                                ‚Ä¢ Sensitive to feature scaling<br>
                                ‚Ä¢ Poor performance on large datasets<br>
                                ‚Ä¢ Sensitive to noise<br>
                                ‚Ä¢ Choice of kernel and parameters crucial
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Kernel SVM Tab -->
                <div id="kernel-svm" class="tab-panel">
                    <h4>üîÑ Kernel SVM</h4>
                    <p>Extends linear SVM to handle non-linear classification by mapping data to higher-dimensional space using kernel functions.</p>

                    <h4>Kernel Trick:</h4>
                    <div class="visual-box">
                        Instead of explicitly mapping œÜ(x), use kernel function:
                        K(x·µ¢, x‚±º) = œÜ(x·µ¢) ¬∑ œÜ(x‚±º)
                        <br><br>
                        This allows computing dot products in high-dimensional space
                        without explicitly transforming the data.
                    </div>

                    <h4>Common Kernels:</h4>
                    <div class="visual-box">
                        <strong>Linear:</strong> K(x,y) = x¬∑y
                        <br><br>
                        <strong>Polynomial:</strong> K(x,y) = (Œ≥x¬∑y + r)^d
                        ‚Ä¢ Œ≥ = kernel coefficient
                        ‚Ä¢ r = independent term
                        ‚Ä¢ d = degree
                        <br><br>
                        <strong>RBF (Gaussian):</strong> K(x,y) = exp(-Œ≥||x-y||¬≤)
                        ‚Ä¢ Œ≥ = kernel coefficient (controls influence radius)
                        <br><br>
                        <strong>Sigmoid:</strong> K(x,y) = tanh(Œ≥x¬∑y + r)
                    </div>

                    <div class="visual-box">
                        <h4>Example: Non-linear Classification</h4>
                        <div class="visual-box">
Problem: Classify points inside/outside a circle

Linear SVM: Cannot separate circular boundary
RBF Kernel SVM: Maps to higher dimension where linear separation possible

Data points:
Point A: (1,1) ‚Üí Inside circle ‚Üí Class +1
Point B: (3,3) ‚Üí Outside circle ‚Üí Class -1
Point C: (0,2) ‚Üí Inside circle ‚Üí Class +1
Point D: (4,1) ‚Üí Outside circle ‚Üí Class -1

RBF Kernel with Œ≥=0.5:
K(A,C) = exp(-0.5√ó||(1,1)-(0,2)||¬≤) = exp(-0.5√ó2) = 0.368
K(A,B) = exp(-0.5√ó||(1,1)-(3,3)||¬≤) = exp(-0.5√ó8) = 0.018

The kernel creates non-linear decision boundary in original space.
                        </div>
                    </div>

                    <h4>Hyperparameter Tuning:</h4>
                    <div class="visual-box">
                        <strong>C (Regularization):</strong>
                        ‚Ä¢ High C: Hard margin, may overfit
                        ‚Ä¢ Low C: Soft margin, may underfit
                        <br><br>
                        <strong>Œ≥ (RBF kernel):</strong>
                        ‚Ä¢ High Œ≥: Tight fit, complex boundary
                        ‚Ä¢ Low Œ≥: Loose fit, simple boundary
                        <br><br>
                        Use cross-validation to find optimal values.
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Handles non-linear relationships<br>
                                ‚Ä¢ Flexible with different kernels<br>
                                ‚Ä¢ Effective in high dimensions<br>
                                ‚Ä¢ Memory efficient<br>
                                ‚Ä¢ Strong theoretical foundation
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Slow on large datasets<br>
                                ‚Ä¢ Sensitive to hyperparameters<br>
                                ‚Ä¢ No probabilistic interpretation<br>
                                ‚Ä¢ Difficult to interpret<br>
                                ‚Ä¢ Requires feature scaling
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Naive Bayes Tab -->
                <div id="naive-bayes" class="tab-panel">
                    <h4>üß† Naive Bayes</h4>
                    <p>Probabilistic classifier based on Bayes' theorem with the "naive" assumption of feature independence.</p>

                    <h4>Bayes' Theorem:</h4>
                    <div class="visual-box">
                        P(Class|Features) = P(Features|Class) √ó P(Class) / P(Features)
                        <br><br>
                        Naive assumption: Features are independent
                        P(x‚ÇÅ,x‚ÇÇ,...,x‚Çô|Class) = P(x‚ÇÅ|Class) √ó P(x‚ÇÇ|Class) √ó ... √ó P(x‚Çô|Class)
                    </div>

                    <h4>Classification Rule:</h4>
                    <div class="visual-box">
                        ≈∑ = argmax P(C‚Çñ) ‚àè P(x·µ¢|C‚Çñ)
                        <br><br>
                        Choose class with highest posterior probability
                    </div>

                    <h4>Types of Naive Bayes:</h4>
                    <div class="visual-box">
                        <strong>Gaussian NB:</strong> Continuous features (normal distribution)
                        P(x·µ¢|C‚Çñ) = (1/‚àö2œÄœÉ‚Çñ¬≤) √ó exp(-(x·µ¢-Œº‚Çñ)¬≤/2œÉ‚Çñ¬≤)
                        <br><br>
                        <strong>Multinomial NB:</strong> Discrete features (word counts)
                        P(x·µ¢|C‚Çñ) = (count(x·µ¢,C‚Çñ) + Œ±) / (count(C‚Çñ) + Œ±√ón)
                        <br><br>
                        <strong>Bernoulli NB:</strong> Binary features (presence/absence)
                        P(x·µ¢|C‚Çñ) = P(x·µ¢=1|C‚Çñ)^x·µ¢ √ó (1-P(x·µ¢=1|C‚Çñ))^(1-x·µ¢)
                    </div>

                    <div class="visual-box">
                        <h4>Example: Text Classification</h4>
                        <div class="visual-box">
Problem: Classify emails as Spam/Ham

Training data:
Spam emails: "buy now", "free money", "click here"
Ham emails: "meeting tomorrow", "project update", "lunch plans"

Word probabilities:
P("free"|Spam) = 0.8, P("free"|Ham) = 0.1
P("money"|Spam) = 0.7, P("money"|Ham) = 0.05
P(Spam) = 0.4, P(Ham) = 0.6

New email: "free money offer"
P(Spam|email) ‚àù P(Spam) √ó P("free"|Spam) √ó P("money"|Spam)
                = 0.4 √ó 0.8 √ó 0.7 = 0.224

P(Ham|email) ‚àù P(Ham) √ó P("free"|Ham) √ó P("money"|Ham)
               = 0.6 √ó 0.1 √ó 0.05 = 0.003

Result: 0.224 > 0.003 ‚Üí Classified as SPAM
                        </div>
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Fast training and prediction<br>
                                ‚Ä¢ Works well with small datasets<br>
                                ‚Ä¢ Handles multi-class naturally<br>
                                ‚Ä¢ Not sensitive to irrelevant features<br>
                                ‚Ä¢ Good baseline classifier
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Strong independence assumption<br>
                                ‚Ä¢ Can be outperformed by more sophisticated methods<br>
                                ‚Ä¢ Requires smoothing for zero probabilities<br>
                                ‚Ä¢ Poor estimator for probability<br>
                                ‚Ä¢ Categorical inputs need Laplace smoothing
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Decision Tree Tab -->
                <div id="decision-tree" class="tab-panel">
                    <h4>üå≥ Decision Tree Classification</h4>
                    <p>Creates a tree-like model of decisions by recursively splitting data based on feature values that best separate classes.</p>

                    <h4>Algorithm:</h4>
                    <div class="visual-box">
                        1. Start with entire dataset at root
                        2. Find best feature and split value
                        3. Split data into branches
                        4. Repeat recursively for each branch
                        5. Stop when stopping criteria met
                        6. Assign majority class to leaf nodes
                    </div>

                    <h4>Splitting Criteria:</h4>
                    <div class="visual-box">
                        <strong>Gini Impurity:</strong>
                        Gini = 1 - Œ£(p·µ¢)¬≤
                        <br><br>
                        <strong>Entropy:</strong>
                        Entropy = -Œ£(p·µ¢ √ó log‚ÇÇ(p·µ¢))
                        <br><br>
                        <strong>Information Gain:</strong>
                        IG = Entropy(parent) - Œ£(w·µ¢ √ó Entropy(child·µ¢))
                        <br><br>
                        Where p·µ¢ = proportion of class i
                    </div>

                    <div class="visual-box">
                        <h4>Example: Loan Approval Decision Tree</h4>
                        <div class="visual-box">
Features: Income, Credit Score, Employment

Root: All applicants (60% approved, 40% rejected)
‚îú‚îÄ‚îÄ Income ‚â§ 50k (20% approved, 80% rejected)
‚îÇ   ‚îú‚îÄ‚îÄ Credit Score ‚â§ 600 ‚Üí REJECT
‚îÇ   ‚îî‚îÄ‚îÄ Credit Score > 600 ‚Üí APPROVE
‚îî‚îÄ‚îÄ Income > 50k (80% approved, 20% rejected)
    ‚îú‚îÄ‚îÄ Employment < 2 years ‚Üí REJECT
    ‚îî‚îÄ‚îÄ Employment ‚â• 2 years ‚Üí APPROVE

New applicant: Income=60k, Credit=650, Employment=3 years
Path: Root ‚Üí Income > 50k ‚Üí Employment ‚â• 2 years ‚Üí APPROVE
                        </div>
                    </div>

                    <h4>Pruning:</h4>
                    <div class="visual-box">
                        <strong>Pre-pruning:</strong> Stop growing early
                        ‚Ä¢ Max depth, min samples per leaf, min impurity decrease
                        <br><br>
                        <strong>Post-pruning:</strong> Remove branches after growing
                        ‚Ä¢ Cost complexity pruning, reduced error pruning
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Easy to understand and interpret<br>
                                ‚Ä¢ No assumptions about data distribution<br>
                                ‚Ä¢ Handles both numerical and categorical<br>
                                ‚Ä¢ Automatic feature selection<br>
                                ‚Ä¢ No need for feature scaling
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Prone to overfitting<br>
                                ‚Ä¢ Unstable (small changes = different tree)<br>
                                ‚Ä¢ Biased toward features with more levels<br>
                                ‚Ä¢ Difficulty with linear relationships<br>
                                ‚Ä¢ Can create overly complex trees
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Random Forest Tab -->
                <div id="random-forest" class="tab-panel">
                    <h4>üå≤ Random Forest Classification</h4>
                    <p>Ensemble method that combines multiple decision trees using bootstrap sampling and random feature selection.</p>

                    <h4>Algorithm:</h4>
                    <div class="visual-box">
                        1. Create n bootstrap samples from training data
                        2. For each sample, train a decision tree with:
                           ‚Ä¢ Random subset of features at each split
                           ‚Ä¢ Different random seed
                        3. For prediction: majority vote of all trees
                        <br><br>
                        Final prediction = mode(tree‚ÇÅ, tree‚ÇÇ, ..., tree‚Çô)
                    </div>

                    <h4>Key Concepts:</h4>
                    <div class="visual-box">
                        <strong>Bootstrap Sampling:</strong> Random sampling with replacement
                        <br><br>
                        <strong>Feature Randomness:</strong> ‚àö(total_features) features per split
                        <br><br>
                        <strong>Bagging:</strong> Bootstrap Aggregating reduces overfitting
                        <br><br>
                        <strong>Out-of-Bag (OOB):</strong> Use unsampled data for validation
                    </div>

                    <div class="visual-box">
                        <h4>Example: Image Classification</h4>
                        <div class="visual-box">
Problem: Classify images as Cat/Dog/Bird

Image features: Color histogram, texture, edges
Training: 1000 images per class

Random Forest with 100 trees:
Tree 1: Trained on bootstrap sample 1, uses features [color, texture]
Tree 2: Trained on bootstrap sample 2, uses features [edges, brightness]
...
Tree 100: Trained on bootstrap sample 100, uses features [contrast, color]

New image prediction:
Tree 1: Cat (confidence: 0.7)
Tree 2: Dog (confidence: 0.6)
Tree 3: Cat (confidence: 0.8)
...
Tree 100: Cat (confidence: 0.9)

Vote count: Cat=65, Dog=25, Bird=10
Final prediction: Cat (majority vote)
                        </div>
                    </div>

                    <h4>Feature Importance:</h4>
                    <div class="visual-box">
                        Importance = Œ£(weighted_impurity_decrease) / n_trees
                        <br><br>
                        Measures how much each feature contributes to decreasing
                        impurity across all trees in the forest.
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Reduces overfitting vs single tree<br>
                                ‚Ä¢ Handles missing values<br>
                                ‚Ä¢ Provides feature importance<br>
                                ‚Ä¢ Works with mixed data types<br>
                                ‚Ä¢ Robust to outliers<br>
                                ‚Ä¢ Parallel training possible
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Less interpretable than single tree<br>
                                ‚Ä¢ Can overfit with very noisy data<br>
                                ‚Ä¢ Biased toward categorical variables<br>
                                ‚Ä¢ Memory intensive<br>
                                ‚Ä¢ Slower prediction than single tree
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2024 My AI Learning Blog</p>
    </footer>

    <!-- Gemini AI Chatbot -->
    <div id="chatbot-container">
        <div id="chatbot-toggle">
            <img src="AI_img-removebg-preview.png" alt="AI" style="width: 40px; height: 40px; border-radius: 50%; object-fit: cover;">
        </div>
        <div id="chatbot-window">
            <div id="chatbot-header">
                <span>AI Assistant</span>
                <div class="chatbot-controls">
                    <button id="chatbot-refresh" class="refresh-btn">
                        <svg viewBox="0 0 24 24">
                            <path d="M17.65 6.35C16.2 4.9 14.21 4 12 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08c-.82 2.33-3.04 4-5.65 4-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"/>
                        </svg>
                    </button>
                    <button id="chatbot-close">√ó</button>
                </div>
            </div>
            <div id="chatbot-messages"></div>
            <div id="chatbot-input-container">
                <input type="text" id="chatbot-input" placeholder="Ask me about AI...">
                <button id="chatbot-send">Send</button>
            </div>
        </div>
    </div>

    <script src="ai-background.js"></script>
    <script src="chatbot-frontend.js"></script>
    <script>
        function showClassificationTab(tabName) {
            // Hide all tab panels
            const panels = document.querySelectorAll('.tab-panel');
            panels.forEach(panel => {
                panel.classList.remove('active');
            });
            
            // Remove active class from all buttons
            const buttons = document.querySelectorAll('.tab-btn');
            buttons.forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected tab panel
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');
        }
    </script>
</body>
</html>