<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regression Guide - My AI Learning Blog</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <nav class="back-nav">
        <button onclick="window.location.href='ai.html'" class="back-btn">‚Üê Back to AI Hub</button>
    </nav>
    
    <header>
        <h1>üìä Regression Guide</h1>
        <p>Complete guide to regression techniques with formulas and examples</p>
    </header>

    <main>
        <!-- Overview Block -->
        <div class="blog-post">
            <div class="topic-header">
                <div class="topic-icon">üéØ</div>
                <div>
                    <h3>What is Regression?</h3>
                </div>
            </div>
            <p>Regression is a statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (features). It's used to predict continuous numerical values.</p>
            
            <div class="visual-box">
                <strong>Key Goal:</strong> Find the best line/curve that fits through data points to make predictions.
            </div>

            <h4>Types of Regression Problems:</h4>
            <ul>
                <li><strong>Simple Linear:</strong> One feature predicts one target</li>
                <li><strong>Multiple Linear:</strong> Multiple features predict one target</li>
                <li><strong>Polynomial:</strong> Non-linear relationships using polynomial terms</li>
                <li><strong>Support Vector:</strong> Uses support vectors and kernels</li>
                <li><strong>Tree-based:</strong> Uses decision trees (Decision Tree, Random Forest)</li>
            </ul>

            <h4>Common Applications:</h4>
            <ul>
                <li>House price prediction</li>
                <li>Stock market forecasting</li>
                <li>Sales revenue estimation</li>
                <li>Temperature prediction</li>
                <li>Medical diagnosis scoring</li>
            </ul>
        </div>

        <!-- Regression Algorithms Tabs -->
        <div class="blog-post">
            <div class="topic-header">
                <div class="topic-icon">üìä</div>
                <div>
                    <h3>Regression Algorithms</h3>
                </div>
            </div>
            
            <div class="algorithm-tabs">
                <button class="tab-btn active" onclick="showRegressionTab('simple-linear')">üìà Simple Linear</button>
                <button class="tab-btn" onclick="showRegressionTab('multiple-linear')">üìä Multiple Linear</button>
                <button class="tab-btn" onclick="showRegressionTab('polynomial')">üåä Polynomial</button>
                <button class="tab-btn" onclick="showRegressionTab('svr')">üéØ SVR</button>
                <button class="tab-btn" onclick="showRegressionTab('decision-tree')">üå≥ Decision Tree</button>
                <button class="tab-btn" onclick="showRegressionTab('random-forest')">üå≤ Random Forest</button>
                <button class="tab-btn" onclick="showRegressionTab('r-squared')">üìè R-Squared</button>
            </div>

            <div class="tab-content">
                <!-- Simple Linear Regression Tab -->
                <div id="simple-linear" class="tab-panel active">
                    <h4>üìà Simple Linear Regression</h4>
                    <p>The simplest form of regression that models the relationship between one independent variable (X) and one dependent variable (Y) using a straight line.</p>

                    <h4>Formula:</h4>
                    <div class="visual-box">
                        Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ
                        <br><br>
                        Where:<br>
                        ‚Ä¢ Y = Dependent variable (target)<br>
                        ‚Ä¢ X = Independent variable (feature)<br>
                        ‚Ä¢ Œ≤‚ÇÄ = Y-intercept (bias)<br>
                        ‚Ä¢ Œ≤‚ÇÅ = Slope (coefficient)<br>
                        ‚Ä¢ Œµ = Error term
                    </div>

                    <h4>Coefficient Calculation:</h4>
                    <div class="visual-box">
                        Œ≤‚ÇÅ = Œ£[(Xi - XÃÑ)(Yi - »≤)] / Œ£[(Xi - XÃÑ)¬≤]
                        <br><br>
                        Œ≤‚ÇÄ = »≤ - Œ≤‚ÇÅXÃÑ
                        <br><br>
                        Where XÃÑ and »≤ are means of X and Y respectively
                    </div>

                    <div class="visual-box">
                        <h4>Example: Predicting House Prices</h4>
                        <p><strong>Problem:</strong> Predict house price based on size (sq ft)</p>
                        <p><strong>Data:</strong></p>
                        <ul>
                            <li>House 1: 1000 sq ft ‚Üí $200,000</li>
                            <li>House 2: 1500 sq ft ‚Üí $300,000</li>
                            <li>House 3: 2000 sq ft ‚Üí $400,000</li>
                        </ul>
                        <p><strong>Result:</strong> Price = 100,000 + 150 √ó Size</p>
                        <p><strong>Prediction:</strong> 1200 sq ft house = $280,000</p>
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Simple to understand and implement<br>
                                ‚Ä¢ Fast computation<br>
                                ‚Ä¢ No hyperparameters to tune<br>
                                ‚Ä¢ Good baseline model<br>
                                ‚Ä¢ Interpretable coefficients
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Assumes linear relationship<br>
                                ‚Ä¢ Sensitive to outliers<br>
                                ‚Ä¢ Only works with one feature<br>
                                ‚Ä¢ May underfit complex data<br>
                                ‚Ä¢ Assumes constant variance
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Multiple Linear Regression Tab -->
                <div id="multiple-linear" class="tab-panel">
                    <h4>üìä Multiple Linear Regression</h4>
                    <p>Extension of simple linear regression that uses multiple independent variables to predict a dependent variable.</p>

                    <h4>Formula:</h4>
                    <div class="visual-box">
                        Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ... + Œ≤‚ÇôX‚Çô + Œµ
                        <br><br>
                        Matrix form: Y = XŒ≤ + Œµ
                        <br><br>
                        Where:<br>
                        ‚Ä¢ Y = Target variable<br>
                        ‚Ä¢ X‚ÇÅ, X‚ÇÇ, ..., X‚Çô = Feature variables<br>
                        ‚Ä¢ Œ≤‚ÇÄ, Œ≤‚ÇÅ, ..., Œ≤‚Çô = Coefficients<br>
                        ‚Ä¢ Œµ = Error term
                    </div>

                    <h4>Coefficient Calculation (Normal Equation):</h4>
                    <div class="visual-box">
                        Œ≤ = (X·µÄX)‚Åª¬πX·µÄY
                        <br><br>
                        Where:<br>
                        ‚Ä¢ X·µÄ = Transpose of X matrix<br>
                        ‚Ä¢ (X·µÄX)‚Åª¬π = Inverse of X·µÄX
                    </div>

                    <div class="visual-box">
                        <h4>Example: House Price with Multiple Features</h4>
                        <p><strong>Features:</strong> Size, Bedrooms, Age</p>
                        <p><strong>Formula:</strong> Price = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óSize + Œ≤‚ÇÇ√óBedrooms + Œ≤‚ÇÉ√óAge</p>
                        <p><strong>Sample Result:</strong></p>
                        <div class="visual-box">
Price = 50,000 + 150√óSize + 10,000√óBedrooms - 2,000√óAge

Example prediction:
Size: 1500 sq ft, Bedrooms: 3, Age: 10 years
Price = 50,000 + 150√ó1500 + 10,000√ó3 - 2,000√ó10
Price = 50,000 + 225,000 + 30,000 - 20,000 = $285,000
                        </div>
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Handles multiple features<br>
                                ‚Ä¢ More accurate than simple linear<br>
                                ‚Ä¢ Still interpretable<br>
                                ‚Ä¢ Fast computation<br>
                                ‚Ä¢ Well-established theory
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Assumes linear relationships<br>
                                ‚Ä¢ Multicollinearity issues<br>
                                ‚Ä¢ Sensitive to outliers<br>
                                ‚Ä¢ Requires feature scaling<br>
                                ‚Ä¢ May overfit with many features
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Polynomial Regression Tab -->
                <div id="polynomial" class="tab-panel">
                    <h4>üåä Polynomial Regression</h4>
                    <p>A form of regression that models non-linear relationships by using polynomial terms of the input features.</p>

                    <h4>Formula:</h4>
                    <div class="visual-box">
                        Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + Œ≤‚ÇÉX¬≥ + ... + Œ≤‚ÇôX‚Åø + Œµ
                        <br><br>
                        For multiple features:<br>
                        Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + Œ≤‚ÇÉX‚ÇÅ¬≤ + Œ≤‚ÇÑX‚ÇÇ¬≤ + Œ≤‚ÇÖX‚ÇÅX‚ÇÇ + ... + Œµ
                        <br><br>
                        Where n is the degree of the polynomial
                    </div>

                    <h4>Feature Transformation:</h4>
                    <div class="visual-box">
                        Original: [X]
                        Degree 2: [1, X, X¬≤]
                        Degree 3: [1, X, X¬≤, X¬≥]
                        <br><br>
                        For 2 features (X‚ÇÅ, X‚ÇÇ) with degree 2:
                        [1, X‚ÇÅ, X‚ÇÇ, X‚ÇÅ¬≤, X‚ÇÅX‚ÇÇ, X‚ÇÇ¬≤]
                    </div>

                    <div class="visual-box">
                        <h4>Example: Temperature vs Ice Cream Sales</h4>
                        <p><strong>Problem:</strong> Sales don't increase linearly with temperature</p>
                        <div class="visual-box">
Linear: Sales = 100 + 5√óTemperature
Polynomial (degree 2): Sales = 50 + 2√óTemperature + 0.1√óTemperature¬≤

At 20¬∞C:
Linear: 100 + 5√ó20 = 200 units
Polynomial: 50 + 2√ó20 + 0.1√ó400 = 130 units

At 35¬∞C:
Linear: 100 + 5√ó35 = 275 units  
Polynomial: 50 + 2√ó35 + 0.1√ó1225 = 242.5 units
                        </div>
                    </div>

                    <div class="visual-box">
                        <strong>Choosing Degree:</strong> Higher degrees can fit training data better but may overfit. Use cross-validation to find optimal degree.
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Captures non-linear relationships<br>
                                ‚Ä¢ Flexible model<br>
                                ‚Ä¢ Still uses linear regression techniques<br>
                                ‚Ä¢ Can model complex curves<br>
                                ‚Ä¢ Good for trend analysis
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Prone to overfitting<br>
                                ‚Ä¢ High computational cost<br>
                                ‚Ä¢ Sensitive to outliers<br>
                                ‚Ä¢ Difficult to interpret<br>
                                ‚Ä¢ Extrapolation problems
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Support Vector Regression Tab -->
                <div id="svr" class="tab-panel">
                    <h4>üéØ Support Vector Regression (SVR)</h4>
                    <p>SVR uses the same principles as SVM but for regression. It tries to fit the best line within a threshold value (epsilon).</p>

                    <h4>Formula:</h4>
                    <div class="visual-box">
                        f(x) = w·µÄœÜ(x) + b
                        <br><br>
                        Objective: Minimize ¬Ω||w||¬≤ + C‚àë(Œæ·µ¢ + Œæ·µ¢*)
                        <br><br>
                        Subject to:
                        ‚Ä¢ y·µ¢ - w·µÄœÜ(x·µ¢) - b ‚â§ Œµ + Œæ·µ¢
                        ‚Ä¢ w·µÄœÜ(x·µ¢) + b - y·µ¢ ‚â§ Œµ + Œæ·µ¢*
                        ‚Ä¢ Œæ·µ¢, Œæ·µ¢* ‚â• 0
                    </div>

                    <h4>Key Concepts:</h4>
                    <div class="visual-box">
                        ‚Ä¢ Œµ (epsilon): Tolerance margin
                        ‚Ä¢ C: Regularization parameter
                        ‚Ä¢ œÜ(x): Kernel function transformation
                        ‚Ä¢ Œæ: Slack variables for points outside Œµ-tube
                    </div>

                    <h4>Common Kernels:</h4>
                    <ul>
                        <li><strong>Linear:</strong> K(x, x') = x ¬∑ x'</li>
                        <li><strong>Polynomial:</strong> K(x, x') = (Œ≥x ¬∑ x' + r)·µà</li>
                        <li><strong>RBF (Gaussian):</strong> K(x, x') = exp(-Œ≥||x - x'||¬≤)</li>
                        <li><strong>Sigmoid:</strong> K(x, x') = tanh(Œ≥x ¬∑ x' + r)</li>
                    </ul>

                    <div class="visual-box">
                        <h4>Example: Stock Price Prediction</h4>
                        <div class="visual-box">
from sklearn.svm import SVR
import numpy as np

# Sample data: [day] -> [price]
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([100, 120, 140, 180, 220])

# SVR with RBF kernel
svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
svr.fit(X, y)

# Predict day 6
prediction = svr.predict([[6]])
print(f"Day 6 price: ${prediction[0]:.2f}")
                        </div>
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Handles non-linear relationships<br>
                                ‚Ä¢ Robust to outliers<br>
                                ‚Ä¢ Works well with high dimensions<br>
                                ‚Ä¢ Memory efficient<br>
                                ‚Ä¢ Versatile (different kernels)
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Slow on large datasets<br>
                                ‚Ä¢ Sensitive to feature scaling<br>
                                ‚Ä¢ No probabilistic output<br>
                                ‚Ä¢ Hard to interpret<br>
                                ‚Ä¢ Many hyperparameters
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Decision Tree Regression Tab -->
                <div id="decision-tree" class="tab-panel">
                    <h4>üå≥ Decision Tree Regression</h4>
                    <p>Uses a tree-like model of decisions to predict continuous values by splitting data based on feature values.</p>

                    <h4>How it Works:</h4>
                    <div class="visual-box">
                        1. Start with entire dataset
                        2. Find best feature and split point that minimizes MSE
                        3. Split data into two branches
                        4. Repeat recursively for each branch
                        5. Stop when criteria met (max depth, min samples, etc.)
                    </div>

                    <h4>Split Criteria (MSE):</h4>
                    <div class="visual-box">
                        MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑)¬≤
                        <br><br>
                        Best split minimizes weighted MSE:
                        MSE_split = (n_left/n) √ó MSE_left + (n_right/n) √ó MSE_right
                    </div>

                    <h4>Prediction:</h4>
                    <div class="visual-box">
                        For each leaf node: ≈∑ = mean(y_values_in_leaf)
                    </div>

                    <div class="visual-box">
                        <h4>Example: House Price Tree</h4>
                        <div class="visual-box">
Root: All houses (avg price: $300k)
‚îú‚îÄ‚îÄ Size ‚â§ 1500 sq ft (avg: $250k)
‚îÇ   ‚îú‚îÄ‚îÄ Bedrooms ‚â§ 2 ‚Üí $200k
‚îÇ   ‚îî‚îÄ‚îÄ Bedrooms > 2 ‚Üí $300k
‚îî‚îÄ‚îÄ Size > 1500 sq ft (avg: $400k)
    ‚îú‚îÄ‚îÄ Age ‚â§ 10 years ‚Üí $450k
    ‚îî‚îÄ‚îÄ Age > 10 years ‚Üí $350k

New house: 1800 sq ft, 3 bedrooms, 5 years old
Path: Root ‚Üí Size > 1500 ‚Üí Age ‚â§ 10 ‚Üí Prediction: $450k
                        </div>
                    </div>

                    <div class="visual-box">
                        <strong>Hyperparameters:</strong> max_depth, min_samples_split, min_samples_leaf, max_features
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Easy to understand and interpret<br>
                                ‚Ä¢ No assumptions about data distribution<br>
                                ‚Ä¢ Handles both numerical and categorical<br>
                                ‚Ä¢ Automatic feature selection<br>
                                ‚Ä¢ No need for feature scaling
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Prone to overfitting<br>
                                ‚Ä¢ Unstable (small data changes = different tree)<br>
                                ‚Ä¢ Biased toward features with more levels<br>
                                ‚Ä¢ Poor extrapolation<br>
                                ‚Ä¢ Can create overly complex trees
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Random Forest Regression Tab -->
                <div id="random-forest" class="tab-panel">
                    <h4>üå≤ Random Forest Regression</h4>
                    <p>An ensemble method that combines multiple decision trees to create a more robust and accurate model.</p>

                    <h4>Algorithm:</h4>
                    <div class="visual-box">
                        1. Create n bootstrap samples from training data
                        2. For each sample, train a decision tree with:
                           ‚Ä¢ Random subset of features at each split
                           ‚Ä¢ Different random seed
                        3. For prediction: average all tree predictions
                        <br><br>
                        ≈∑ = (1/n) √ó Œ£(tree_i_prediction)
                    </div>

                    <h4>Key Concepts:</h4>
                    <ul>
                        <li><strong>Bootstrap Sampling:</strong> Random sampling with replacement</li>
                        <li><strong>Feature Randomness:</strong> Only consider subset of features per split</li>
                        <li><strong>Bagging:</strong> Bootstrap Aggregating reduces overfitting</li>
                        <li><strong>Out-of-Bag (OOB):</strong> Use unsampled data for validation</li>
                    </ul>

                    <div class="visual-box">
                        <h4>Example: Ensemble Prediction</h4>
                        <div class="visual-box">
House: 1600 sq ft, 3 bedrooms, 8 years old

Tree 1 prediction: $320k (trained on sample 1)
Tree 2 prediction: $340k (trained on sample 2)  
Tree 3 prediction: $310k (trained on sample 3)
Tree 4 prediction: $330k (trained on sample 4)
Tree 5 prediction: $325k (trained on sample 5)

Random Forest prediction: 
(320 + 340 + 310 + 330 + 325) / 5 = $325k
                        </div>
                    </div>

                    <h4>Feature Importance:</h4>
                    <div class="visual-box">
                        Importance = Œ£(weighted_impurity_decrease) / n_trees
                        <br><br>
                        Higher values indicate more important features
                    </div>

                    <div class="concept-visual">
                        <div class="learning-type">
                            <h4>‚úÖ Advantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Reduces overfitting vs single tree<br>
                                ‚Ä¢ Handles missing values<br>
                                ‚Ä¢ Provides feature importance<br>
                                ‚Ä¢ Works with mixed data types<br>
                                ‚Ä¢ Robust to outliers<br>
                                ‚Ä¢ Parallel training possible
                            </div>
                        </div>
                        <div class="learning-type">
                            <h4>‚ùå Disadvantages</h4>
                            <div class="visual-box">
                                ‚Ä¢ Less interpretable than single tree<br>
                                ‚Ä¢ Can overfit with very noisy data<br>
                                ‚Ä¢ Biased toward categorical variables<br>
                                ‚Ä¢ Memory intensive<br>
                                ‚Ä¢ Slower prediction than single tree
                            </div>
                        </div>
                    </div>
                </div>

                <!-- R-Squared Tab -->
                <div id="r-squared" class="tab-panel">
                    <h4>üìè R-Squared (Coefficient of Determination)</h4>
                    <p>R-squared measures how well the regression model explains the variability in the target variable. It's the most common metric for evaluating regression models.</p>

                    <h4>Formula:</h4>
                    <div class="visual-box">
                        R¬≤ = 1 - (SS_res / SS_tot)
                        <br><br>
                        Where:<br>
                        ‚Ä¢ SS_res = Œ£(y·µ¢ - ≈∑·µ¢)¬≤ (Residual Sum of Squares)<br>
                        ‚Ä¢ SS_tot = Œ£(y·µ¢ - »≥)¬≤ (Total Sum of Squares)<br>
                        ‚Ä¢ y·µ¢ = Actual values<br>
                        ‚Ä¢ ≈∑·µ¢ = Predicted values<br>
                        ‚Ä¢ »≥ = Mean of actual values
                    </div>

                    <h4>Alternative Formula:</h4>
                    <div class="visual-box">
                        R¬≤ = (SS_reg / SS_tot)
                        <br><br>
                        Where SS_reg = Œ£(≈∑·µ¢ - »≥)¬≤ (Regression Sum of Squares)
                        <br><br>
                        Note: SS_tot = SS_reg + SS_res
                    </div>

                    <h4>Interpretation:</h4>
                    <ul>
                        <li><strong>R¬≤ = 1.0:</strong> Perfect fit (model explains 100% of variance)</li>
                        <li><strong>R¬≤ = 0.8:</strong> Good fit (model explains 80% of variance)</li>
                        <li><strong>R¬≤ = 0.5:</strong> Moderate fit (model explains 50% of variance)</li>
                        <li><strong>R¬≤ = 0.0:</strong> Model performs as well as predicting the mean</li>
                        <li><strong>R¬≤ < 0:</strong> Model performs worse than predicting the mean</li>
                    </ul>

                    <div class="visual-box">
                        <h4>Example Calculation:</h4>
                        <div class="visual-box">
Actual values: [100, 120, 140, 160, 180]
Predicted values: [110, 125, 135, 155, 175]
Mean of actual: 140

SS_res = (100-110)¬≤ + (120-125)¬≤ + (140-135)¬≤ + (160-155)¬≤ + (180-175)¬≤
       = 100 + 25 + 25 + 25 + 25 = 200

SS_tot = (100-140)¬≤ + (120-140)¬≤ + (140-140)¬≤ + (160-140)¬≤ + (180-140)¬≤
       = 1600 + 400 + 0 + 400 + 1600 = 4000

R¬≤ = 1 - (200/4000) = 1 - 0.05 = 0.95

Interpretation: Model explains 95% of the variance
                        </div>
                    </div>

                    <h4>Adjusted R-Squared:</h4>
                    <div class="visual-box">
                        Adjusted R¬≤ = 1 - [(1-R¬≤)(n-1)/(n-k-1)]
                        <br><br>
                        Where:<br>
                        ‚Ä¢ n = Number of observations<br>
                        ‚Ä¢ k = Number of features<br>
                        <br>
                        Penalizes adding irrelevant features
                    </div>

                    <div class="visual-box">
                        <strong>When to Use:</strong><br>
                        ‚Ä¢ Comparing different regression models<br>
                        ‚Ä¢ Understanding model performance<br>
                        ‚Ä¢ Feature selection (higher R¬≤ with fewer features is better)<br>
                        ‚Ä¢ Model validation and improvement
                    </div>

                    <h4>Limitations:</h4>
                    <ul>
                        <li>Can be misleading with non-linear relationships</li>
                        <li>Doesn't indicate if model is appropriate</li>
                        <li>High R¬≤ doesn't guarantee good predictions</li>
                        <li>Sensitive to outliers</li>
                        <li>Always increases with more features (use Adjusted R¬≤)</li>
                    </ul>

                    <h4>Other Regression Metrics:</h4>
                    <div class="visual-box">
                        MAE = (1/n) √ó Œ£|y·µ¢ - ≈∑·µ¢| (Mean Absolute Error)
                        <br><br>
                        MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤ (Mean Squared Error)
                        <br><br>
                        RMSE = ‚àöMSE (Root Mean Squared Error)
                    </div>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2024 My AI Learning Blog</p>
    </footer>

    <!-- Gemini AI Chatbot -->
    <div id="chatbot-container">
        <div id="chatbot-toggle">
            <img src="AI_img-removebg-preview.png" alt="AI" style="width: 40px; height: 40px; border-radius: 50%; object-fit: cover;">
        </div>
        <div id="chatbot-window">
            <div id="chatbot-header">
                <span>AI Assistant</span>
                <div class="chatbot-controls">
                    <button id="chatbot-refresh" class="refresh-btn">
                        <svg viewBox="0 0 24 24">
                            <path d="M17.65 6.35C16.2 4.9 14.21 4 12 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08c-.82 2.33-3.04 4-5.65 4-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"/>
                        </svg>
                    </button>
                    <button id="chatbot-close">√ó</button>
                </div>
            </div>
            <div id="chatbot-messages"></div>
            <div id="chatbot-input-container">
                <input type="text" id="chatbot-input" placeholder="Ask me about AI...">
                <button id="chatbot-send">Send</button>
            </div>
        </div>
    </div>

    <script src="ai-background.js"></script>
    <script src="chatbot-frontend.js"></script>
    <script>
        function showRegressionTab(tabName) {
            // Hide all tab panels
            const panels = document.querySelectorAll('.tab-panel');
            panels.forEach(panel => {
                panel.classList.remove('active');
            });
            
            // Remove active class from all buttons
            const buttons = document.querySelectorAll('.tab-btn');
            buttons.forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected tab panel
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');
        }
    </script>
</body>
</html>